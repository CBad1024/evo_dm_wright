{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa898df",
   "metadata": {
    "id": "bfa898df"
   },
   "source": "Project file using Wright Fischer model. All the necessary imports are done below."
  },
  {
   "cell_type": "code",
   "id": "d6c8f50a",
   "metadata": {
    "id": "d6c8f50a"
   },
   "source": [
    "!git clone https://github.com/CBad1024/evo_dm_wright.git\n",
    "\n",
    "import os\n",
    "os.chdir('evo_dm_wright')\n",
    "\n",
    "!pip install -e .\n",
    "\n",
    "from evodm.dpsolve import dp_env, backwards_induction, value_iteration, policy_iteration\n",
    "from evodm.evol_game import define_mira_landscapes, evol_env\n",
    "from evodm.learner import DrugSelector, hyperparameters, practice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mira_mdp import mira_env, get_sequences\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50b6e0ff",
   "metadata": {
    "id": "50b6e0ff"
   },
   "source": [
    "Main Code here"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd087a2b",
   "metadata": {
    "id": "fd087a2b"
   },
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to solve the MIRA MDP and evaluate the policies.\n",
    "    \"\"\"\n",
    "    print(\"Initializing MIRA environments (DP and Simulation)...\")\n",
    "    envdp, env, learner_env, learner_env_naive = mira_env() # Removed naive_learner_env from unpack\n",
    "\n",
    "    # --- Solve the MDP using different algorithms ---\n",
    "    print(\"\\nSolving MDP with Backwards Induction (Finite Horizon)...\")\n",
    "    policy_bi, V_bi = backwards_induction(envdp, num_steps=16)\n",
    "    print(\"Policy shape from Backwards Induction:\", policy_bi.shape)\n",
    "\n",
    "    print(\"\\nSolving MDP with Value Iteration...\")\n",
    "    policy_vi, V_vi = value_iteration(envdp)\n",
    "    print(\"Policy shape from Value Iteration:\", policy_vi)\n",
    "\n",
    "    print(\"\\nSolving MDP with Policy Iteration...\")\n",
    "    policy_pi, V_pi = policy_iteration(envdp)\n",
    "    print(\"Policy shape from Policy Iteration:\", policy_pi)\n",
    "\n",
    "    # --- RL Agent Training  ---\n",
    "    print(\"\\nUsing non-naive RL to solve system:\")\n",
    "\n",
    "    results = practice(learner_env, naive=False, standard_practice=True, prev_action= True, num_episodes=50)\n",
    "    agent_NN = results[1]\n",
    "    policy_NN = agent_NN.compute_implied_policy(update = True)\n",
    "    print(\"policy shape under non-naive RL: \", policy_NN)\n",
    "\n",
    "    print(\"\\nUsing naive RL agent to solve system...\")\n",
    "    results = practice(learner_env_naive, naive=True, standard_practice=True, prev_action = True)\n",
    "    agent_N = results[1]\n",
    "    policy_N = agent_N.compute_implied_policy(update = True)\n",
    "    print(\"policy shape under naive RL: \", policy_N)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Evaluate the policies by simulation ---\n",
    "    print(\"\\nSimulating policy from Backwards Induction...\")\n",
    "    bi_results = get_sequences(policy_bi, env, num_episodes=5, episode_length=envdp.nS, finite_horizon=True)\n",
    "    print(\"Backwards Induction Results (first 5 rows):\")\n",
    "    print(bi_results.to_string())\n",
    "    print(\"\\nAverage fitness under BI policy:\", bi_results['fitness'].mean())\n",
    "\n",
    "    print(\"\\nSimulating policy from Value Iteration...\")\n",
    "    vi_results = get_sequences(policy_vi, env, num_episodes=5, episode_length=envdp.nS, finite_horizon=False)\n",
    "    print(\"Value Iteration Results (first 5 rows):\")\n",
    "    print(vi_results.to_string())\n",
    "    print(\"\\nAverage fitness under VI policy:\", vi_results['fitness'].mean())\n",
    "\n",
    "    print(\"\\nSimulating policy from Policy Iteration...\")\n",
    "    pi_results = get_sequences(policy_pi, env, num_episodes=5, episode_length=envdp.nS, finite_horizon=False)\n",
    "    print(\"Policy Iteration Results (first 5 rows):\")\n",
    "    print(pi_results.to_string())\n",
    "    print(\"\\nAverage fitness under PI policy:\", pi_results['fitness'].mean())\n",
    "\n",
    "\n",
    "    # print(\"\\nSimulating policy from Non-naive RL...\")\n",
    "    # RL_NN_results = get_sequences(policy_NN, env, num_episodes = 5, episode_length=envdp.nS, finite_horizon=True)\n",
    "    # print(\"RL NN results:\")\n",
    "    # print(RL_NN_results.to_string())\n",
    "    # print(\"\\nAverage fitness under RL_NN policy:\", RL_NN_results['fitness'].mean())\n",
    "    #\n",
    "    # print(\"\\nSimulating policy from naive RL...\")\n",
    "    # RL_N_results = get_sequences(policy_N, env, num_episodes = 5, episode_length=envdp.nS, finite_horizon=True)\n",
    "    # print(\"RL NN results:\")\n",
    "    # print(RL_N_results.to_string())\n",
    "    # print(\"\\nAverage fitness under RL_NN policy:\", RL_N_results['fitness'].mean())\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now call the main code for execution",
   "id": "f67f0f1be96d82bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "main()",
   "id": "72b98aec"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
